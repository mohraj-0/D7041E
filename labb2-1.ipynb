{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mohammad Jawad Rajabi\n",
    "mohraj-0@student.ltu.se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim,logging, numpy as np\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\jawad\\OneDrive\\Skrivbord\\Lab2\\Lab2\\GensimW2V\\help_functions\")\n",
    "sys.path.append(r\"C:\\Users\\jawad\\OneDrive\\Skrivbord\\Lab2\\Lab2\\GensimW2V\\text_functions\")\n",
    "import nltk\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laddar ner punktueringsmodulen, som används för att dela upp text i meningar eller ord\n",
    "Laddar ner WordNet, en databas för ordrelationer som används för lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jawad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jawad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Konfigurerar loggning för att visa tidsstämpel och meddelanden. Skapar en lemmatizer för att reducera ord till sina grundformer. Läser in och splittar lemmatizerad text från filen till listan sentences, där varje rad blir en lista av ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = r\"c:\\Users\\jawad\\OneDrive\\Skrivbord\\Lab2\\Lab2\\GensimW2V\\lemmatized.text\"\n",
    "new_toefl_txt = r\"c:\\Users\\jawad\\OneDrive\\Skrivbord\\Lab2\\Lab2\\GensimW2V\\new_toefl.txt\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "sentences = []\n",
    "with open(lemmatized_text,\"r\", encoding=\"utf-8\") as file :\n",
    "    for line in file:\n",
    "        sentences.append(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funktionen tränar en Word2Vec-modell med olika dimensioner (10, 100, 1000) och testar den på TOEFL-uppgifter och beräknar hur många svar som är rätt och visar resultaten som noggrannhet för varje dimension. Till slut ger en sammanfattning av resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 10:20:37,329 : INFO : collecting all words and their counts\n",
      "2024-11-23 10:20:37,333 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 10:20:37,381 : INFO : PROGRESS: at sentence #10000, processed 112555 words, keeping 6354 word types\n",
      "2024-11-23 10:20:37,436 : INFO : PROGRESS: at sentence #20000, processed 254983 words, keeping 10821 word types\n",
      "2024-11-23 10:20:37,485 : INFO : PROGRESS: at sentence #30000, processed 367772 words, keeping 12635 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- experiments for dimension10\n",
      "\n",
      "Running Word2Vec with vector dimension 10, Run1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 10:20:37,559 : INFO : PROGRESS: at sentence #40000, processed 566573 words, keeping 17685 word types\n",
      "2024-11-23 10:20:37,617 : INFO : PROGRESS: at sentence #50000, processed 724294 words, keeping 20496 word types\n",
      "2024-11-23 10:20:37,683 : INFO : PROGRESS: at sentence #60000, processed 869309 words, keeping 22253 word types\n",
      "2024-11-23 10:20:37,738 : INFO : PROGRESS: at sentence #70000, processed 1006992 words, keeping 23483 word types\n",
      "2024-11-23 10:20:37,788 : INFO : PROGRESS: at sentence #80000, processed 1138953 words, keeping 24604 word types\n",
      "2024-11-23 10:20:37,839 : INFO : PROGRESS: at sentence #90000, processed 1271320 words, keeping 25783 word types\n",
      "2024-11-23 10:20:37,883 : INFO : PROGRESS: at sentence #100000, processed 1381760 words, keeping 26648 word types\n",
      "2024-11-23 10:20:37,936 : INFO : PROGRESS: at sentence #110000, processed 1513702 words, keeping 27511 word types\n",
      "2024-11-23 10:20:37,982 : INFO : PROGRESS: at sentence #120000, processed 1631919 words, keeping 28085 word types\n",
      "2024-11-23 10:20:38,033 : INFO : PROGRESS: at sentence #130000, processed 1765792 words, keeping 28818 word types\n",
      "2024-11-23 10:20:38,092 : INFO : PROGRESS: at sentence #140000, processed 1923924 words, keeping 30297 word types\n",
      "2024-11-23 10:20:38,137 : INFO : PROGRESS: at sentence #150000, processed 2044830 words, keeping 31192 word types\n",
      "2024-11-23 10:20:38,182 : INFO : PROGRESS: at sentence #160000, processed 2160734 words, keeping 31656 word types\n",
      "2024-11-23 10:20:38,235 : INFO : PROGRESS: at sentence #170000, processed 2293382 words, keeping 32392 word types\n",
      "2024-11-23 10:20:38,294 : INFO : PROGRESS: at sentence #180000, processed 2439256 words, keeping 33343 word types\n",
      "2024-11-23 10:20:38,358 : INFO : PROGRESS: at sentence #190000, processed 2585994 words, keeping 34347 word types\n",
      "2024-11-23 10:20:38,421 : INFO : PROGRESS: at sentence #200000, processed 2738528 words, keeping 35649 word types\n",
      "2024-11-23 10:20:38,488 : INFO : PROGRESS: at sentence #210000, processed 2879787 words, keeping 36320 word types\n",
      "2024-11-23 10:20:38,547 : INFO : PROGRESS: at sentence #220000, processed 3007767 words, keeping 36835 word types\n",
      "2024-11-23 10:20:38,627 : INFO : PROGRESS: at sentence #230000, processed 3153989 words, keeping 38409 word types\n",
      "2024-11-23 10:20:38,701 : INFO : PROGRESS: at sentence #240000, processed 3295498 words, keeping 39637 word types\n",
      "2024-11-23 10:20:38,786 : INFO : PROGRESS: at sentence #250000, processed 3473197 words, keeping 40965 word types\n",
      "2024-11-23 10:20:38,839 : INFO : PROGRESS: at sentence #260000, processed 3590526 words, keeping 41478 word types\n",
      "2024-11-23 10:20:38,883 : INFO : PROGRESS: at sentence #270000, processed 3709916 words, keeping 41846 word types\n",
      "2024-11-23 10:20:38,959 : INFO : PROGRESS: at sentence #280000, processed 3882011 words, keeping 43008 word types\n",
      "2024-11-23 10:20:39,030 : INFO : PROGRESS: at sentence #290000, processed 4039240 words, keeping 43781 word types\n",
      "2024-11-23 10:20:39,099 : INFO : PROGRESS: at sentence #300000, processed 4197239 words, keeping 44266 word types\n",
      "2024-11-23 10:20:39,148 : INFO : PROGRESS: at sentence #310000, processed 4330734 words, keeping 44774 word types\n",
      "2024-11-23 10:20:39,210 : INFO : PROGRESS: at sentence #320000, processed 4505548 words, keeping 46061 word types\n",
      "2024-11-23 10:20:39,267 : INFO : PROGRESS: at sentence #330000, processed 4668049 words, keeping 47185 word types\n",
      "2024-11-23 10:20:39,322 : INFO : PROGRESS: at sentence #340000, processed 4783463 words, keeping 47544 word types\n",
      "2024-11-23 10:20:39,400 : INFO : PROGRESS: at sentence #350000, processed 4913176 words, keeping 47916 word types\n",
      "2024-11-23 10:20:39,501 : INFO : PROGRESS: at sentence #360000, processed 5086888 words, keeping 48701 word types\n",
      "2024-11-23 10:20:39,568 : INFO : PROGRESS: at sentence #370000, processed 5244035 words, keeping 49243 word types\n",
      "2024-11-23 10:20:39,648 : INFO : PROGRESS: at sentence #380000, processed 5381703 words, keeping 49673 word types\n",
      "2024-11-23 10:20:39,727 : INFO : PROGRESS: at sentence #390000, processed 5521379 words, keeping 50071 word types\n",
      "2024-11-23 10:20:39,809 : INFO : PROGRESS: at sentence #400000, processed 5643024 words, keeping 50558 word types\n",
      "2024-11-23 10:20:39,870 : INFO : PROGRESS: at sentence #410000, processed 5762211 words, keeping 50966 word types\n",
      "2024-11-23 10:20:39,932 : INFO : PROGRESS: at sentence #420000, processed 5902815 words, keeping 51294 word types\n",
      "2024-11-23 10:20:39,996 : INFO : PROGRESS: at sentence #430000, processed 6047861 words, keeping 51762 word types\n",
      "2024-11-23 10:20:40,050 : INFO : PROGRESS: at sentence #440000, processed 6169325 words, keeping 52046 word types\n",
      "2024-11-23 10:20:40,110 : INFO : PROGRESS: at sentence #450000, processed 6318470 words, keeping 52740 word types\n",
      "2024-11-23 10:20:40,188 : INFO : PROGRESS: at sentence #460000, processed 6533393 words, keeping 54044 word types\n",
      "2024-11-23 10:20:40,266 : INFO : PROGRESS: at sentence #470000, processed 6719789 words, keeping 54950 word types\n",
      "2024-11-23 10:20:40,313 : INFO : PROGRESS: at sentence #480000, processed 6850469 words, keeping 55246 word types\n",
      "2024-11-23 10:20:40,367 : INFO : PROGRESS: at sentence #490000, processed 7013052 words, keeping 56021 word types\n",
      "2024-11-23 10:20:40,422 : INFO : PROGRESS: at sentence #500000, processed 7167853 words, keeping 56683 word types\n",
      "2024-11-23 10:20:40,466 : INFO : PROGRESS: at sentence #510000, processed 7293581 words, keeping 57203 word types\n",
      "2024-11-23 10:20:40,521 : INFO : PROGRESS: at sentence #520000, processed 7423166 words, keeping 57812 word types\n",
      "2024-11-23 10:20:40,570 : INFO : PROGRESS: at sentence #530000, processed 7557394 words, keeping 58545 word types\n",
      "2024-11-23 10:20:40,642 : INFO : PROGRESS: at sentence #540000, processed 7759813 words, keeping 59416 word types\n",
      "2024-11-23 10:20:40,721 : INFO : PROGRESS: at sentence #550000, processed 7923494 words, keeping 59982 word types\n",
      "2024-11-23 10:20:40,780 : INFO : PROGRESS: at sentence #560000, processed 8078968 words, keeping 61074 word types\n",
      "2024-11-23 10:20:40,830 : INFO : PROGRESS: at sentence #570000, processed 8220916 words, keeping 61572 word types\n",
      "2024-11-23 10:20:40,872 : INFO : PROGRESS: at sentence #580000, processed 8337894 words, keeping 61863 word types\n",
      "2024-11-23 10:20:40,920 : INFO : PROGRESS: at sentence #590000, processed 8467527 words, keeping 62192 word types\n",
      "2024-11-23 10:20:40,971 : INFO : PROGRESS: at sentence #600000, processed 8601730 words, keeping 62628 word types\n",
      "2024-11-23 10:20:41,027 : INFO : PROGRESS: at sentence #610000, processed 8757627 words, keeping 63211 word types\n",
      "2024-11-23 10:20:41,077 : INFO : PROGRESS: at sentence #620000, processed 8899486 words, keeping 63809 word types\n",
      "2024-11-23 10:20:41,143 : INFO : PROGRESS: at sentence #630000, processed 9081782 words, keeping 64483 word types\n",
      "2024-11-23 10:20:41,193 : INFO : PROGRESS: at sentence #640000, processed 9203773 words, keeping 64821 word types\n",
      "2024-11-23 10:20:41,256 : INFO : PROGRESS: at sentence #650000, processed 9389002 words, keeping 65876 word types\n",
      "2024-11-23 10:20:41,314 : INFO : PROGRESS: at sentence #660000, processed 9554591 words, keeping 66413 word types\n",
      "2024-11-23 10:20:41,368 : INFO : PROGRESS: at sentence #670000, processed 9702648 words, keeping 67288 word types\n",
      "2024-11-23 10:20:41,411 : INFO : PROGRESS: at sentence #680000, processed 9827973 words, keeping 67596 word types\n",
      "2024-11-23 10:20:41,453 : INFO : PROGRESS: at sentence #690000, processed 9953739 words, keeping 67868 word types\n",
      "2024-11-23 10:20:41,501 : INFO : PROGRESS: at sentence #700000, processed 10083832 words, keeping 68162 word types\n",
      "2024-11-23 10:20:41,556 : INFO : PROGRESS: at sentence #710000, processed 10223733 words, keeping 68816 word types\n",
      "2024-11-23 10:20:41,623 : INFO : PROGRESS: at sentence #720000, processed 10360488 words, keeping 69381 word types\n",
      "2024-11-23 10:20:41,684 : INFO : PROGRESS: at sentence #730000, processed 10479153 words, keeping 69774 word types\n",
      "2024-11-23 10:20:41,737 : INFO : PROGRESS: at sentence #740000, processed 10597851 words, keeping 70112 word types\n",
      "2024-11-23 10:20:41,787 : INFO : PROGRESS: at sentence #750000, processed 10734438 words, keeping 70523 word types\n",
      "2024-11-23 10:20:41,798 : INFO : collected 70583 word types from a corpus of 10766283 raw words and 752149 sentences\n",
      "2024-11-23 10:20:41,799 : INFO : Creating a fresh vocabulary\n",
      "2024-11-23 10:20:42,033 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 70583 unique words (100.00% of original 70583, drops 0)', 'datetime': '2024-11-23T10:20:42.033034', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-11-23 10:20:42,034 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 10766283 word corpus (100.00% of original 10766283, drops 0)', 'datetime': '2024-11-23T10:20:42.034383', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-11-23 10:20:42,422 : INFO : deleting the raw counts dictionary of 70583 items\n",
      "2024-11-23 10:20:42,424 : INFO : sample=0.00055 downsamples 87 most-common words\n",
      "2024-11-23 10:20:42,425 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7342202.428809431 word corpus (68.2%% of prior 10766283)', 'datetime': '2024-11-23T10:20:42.425657', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-11-23 10:20:43,066 : INFO : estimated required memory for 70583 words and 10 dimensions: 40938140 bytes\n",
      "2024-11-23 10:20:43,068 : INFO : resetting layer weights\n",
      "2024-11-23 10:20:43,072 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-23T10:20:43.072662', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-11-23 10:20:43,074 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 70583 vocabulary and 10 features, using sg=1 hs=0 sample=0.00055 negative=5 window=5 shrink_windows=True', 'datetime': '2024-11-23T10:20:43.074869', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-11-23 10:20:44,100 : INFO : EPOCH 0 - PROGRESS: at 8.41% examples, 610315 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:45,125 : INFO : EPOCH 0 - PROGRESS: at 18.10% examples, 624679 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:46,130 : INFO : EPOCH 0 - PROGRESS: at 26.95% examples, 624968 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:47,146 : INFO : EPOCH 0 - PROGRESS: at 37.26% examples, 653301 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:48,150 : INFO : EPOCH 0 - PROGRESS: at 48.16% examples, 690056 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:49,151 : INFO : EPOCH 0 - PROGRESS: at 60.04% examples, 714234 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:50,155 : INFO : EPOCH 0 - PROGRESS: at 70.63% examples, 731784 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:51,168 : INFO : EPOCH 0 - PROGRESS: at 80.45% examples, 733219 words/s, in_qsize 4, out_qsize 1\n",
      "2024-11-23 10:20:52,183 : INFO : EPOCH 0 - PROGRESS: at 90.08% examples, 734315 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:52,980 : INFO : EPOCH 0: training on 10766283 raw words (7342092 effective words) took 9.9s, 742162 effective words/s\n",
      "2024-11-23 10:20:53,987 : INFO : EPOCH 1 - PROGRESS: at 10.76% examples, 780061 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:54,989 : INFO : EPOCH 1 - PROGRESS: at 23.56% examples, 816111 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:55,992 : INFO : EPOCH 1 - PROGRESS: at 34.76% examples, 818399 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:56,993 : INFO : EPOCH 1 - PROGRESS: at 46.21% examples, 829293 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:57,994 : INFO : EPOCH 1 - PROGRESS: at 58.22% examples, 836136 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:20:58,996 : INFO : EPOCH 1 - PROGRESS: at 68.65% examples, 836363 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:00,007 : INFO : EPOCH 1 - PROGRESS: at 79.85% examples, 836902 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:01,010 : INFO : EPOCH 1 - PROGRESS: at 90.33% examples, 834907 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:01,755 : INFO : EPOCH 1: training on 10766283 raw words (7341568 effective words) took 8.8s, 837135 effective words/s\n",
      "2024-11-23 10:21:02,768 : INFO : EPOCH 2 - PROGRESS: at 11.32% examples, 818671 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:03,770 : INFO : EPOCH 2 - PROGRESS: at 24.43% examples, 845256 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:04,770 : INFO : EPOCH 2 - PROGRESS: at 35.76% examples, 838413 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:05,775 : INFO : EPOCH 2 - PROGRESS: at 46.76% examples, 839820 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:06,776 : INFO : EPOCH 2 - PROGRESS: at 58.32% examples, 836453 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:07,785 : INFO : EPOCH 2 - PROGRESS: at 68.65% examples, 834675 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:08,794 : INFO : EPOCH 2 - PROGRESS: at 79.77% examples, 834504 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:09,796 : INFO : EPOCH 2 - PROGRESS: at 90.08% examples, 831216 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:10,624 : INFO : EPOCH 2: training on 10766283 raw words (7340745 effective words) took 8.9s, 828282 effective words/s\n",
      "2024-11-23 10:21:11,651 : INFO : EPOCH 3 - PROGRESS: at 9.23% examples, 670013 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:12,664 : INFO : EPOCH 3 - PROGRESS: at 20.99% examples, 718946 words/s, in_qsize 6, out_qsize 0\n",
      "2024-11-23 10:21:13,665 : INFO : EPOCH 3 - PROGRESS: at 31.54% examples, 735381 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:14,671 : INFO : EPOCH 3 - PROGRESS: at 41.73% examples, 741584 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:15,681 : INFO : EPOCH 3 - PROGRESS: at 51.68% examples, 743509 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:16,688 : INFO : EPOCH 3 - PROGRESS: at 61.54% examples, 743037 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:17,699 : INFO : EPOCH 3 - PROGRESS: at 71.37% examples, 743125 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:18,701 : INFO : EPOCH 3 - PROGRESS: at 81.29% examples, 742937 words/s, in_qsize 6, out_qsize 0\n",
      "2024-11-23 10:21:19,706 : INFO : EPOCH 3 - PROGRESS: at 90.95% examples, 743062 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:20,457 : INFO : EPOCH 3: training on 10766283 raw words (7341462 effective words) took 9.8s, 747511 effective words/s\n",
      "2024-11-23 10:21:21,463 : INFO : EPOCH 4 - PROGRESS: at 10.21% examples, 746859 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:22,473 : INFO : EPOCH 4 - PROGRESS: at 22.39% examples, 769702 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:23,475 : INFO : EPOCH 4 - PROGRESS: at 32.63% examples, 764618 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:24,483 : INFO : EPOCH 4 - PROGRESS: at 42.49% examples, 762786 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:25,485 : INFO : EPOCH 4 - PROGRESS: at 52.90% examples, 762084 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:26,499 : INFO : EPOCH 4 - PROGRESS: at 62.64% examples, 761288 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:27,501 : INFO : EPOCH 4 - PROGRESS: at 72.28% examples, 759652 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:28,509 : INFO : EPOCH 4 - PROGRESS: at 82.83% examples, 759938 words/s, in_qsize 6, out_qsize 0\n",
      "2024-11-23 10:21:29,528 : INFO : EPOCH 4 - PROGRESS: at 92.75% examples, 756308 words/s, in_qsize 5, out_qsize 0\n",
      "2024-11-23 10:21:30,147 : INFO : EPOCH 4: training on 10766283 raw words (7342789 effective words) took 9.7s, 758169 effective words/s\n",
      "2024-11-23 10:21:30,148 : INFO : Word2Vec lifecycle event {'msg': 'training on 53831415 raw words (36708656 effective words) took 47.1s, 779825 effective words/s', 'datetime': '2024-11-23T10:21:30.148830', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-11-23 10:21:30,149 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=70583, vector_size=10, alpha=0.025>', 'datetime': '2024-11-23T10:21:30.149938', 'gensim': '4.3.3', 'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m dimensions:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- experiments for dimension\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_word2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     results[dim] \u001b[38;5;241m=\u001b[39m accuracies\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m summery of result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mtrain_word2vec\u001b[1;34m(dimension, threshold, num_runs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m words[k] \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mwv:\n\u001b[0;32m     26\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     vectors\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(dimension))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_word2vec(dimension, threshold, num_runs = 5):\n",
    "    accuracies = []\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\nRunning Word2Vec with vector dimension {dimension}, Run{run+1}/{num_runs}\")\n",
    "        model = gensim.models.Word2Vec(sentences, vector_size= dimension, min_count=1, sample = threshold, sg =1, epochs =5)\n",
    "        \n",
    "        right_answer = 0\n",
    "        number_tests = 80\n",
    "        number_skipped = 0\n",
    "        \n",
    "        with open(new_toefl_txt, 'r') as text_file:\n",
    "            for i in range(number_tests):\n",
    "                line = text_file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                words = line.split()\n",
    "                \n",
    "                try:\n",
    "                    words =[lemmatizer.lemmatize(word,pos) for word in words for pos in('v','n', 'a')]\n",
    "                    vectors = []\n",
    "                    if words[0] in model.wv:\n",
    "                        k = 1\n",
    "                        vectors.append(model.wv[words[0]])\n",
    "                        while k <5:\n",
    "                            if words[k] in model.wv:\n",
    "                                k = 1\n",
    "                                vectors.append(model.wv[words[k]])\n",
    "                            else:\n",
    "                                vectors.append(np.random.randn(dimension))\n",
    "                            k +=1\n",
    "                        right_answer += hf.get_answer_mod(vectors)\n",
    "                    else:\n",
    "                        number_skipped +=1\n",
    "                except KeyError:\n",
    "                    number_tests +=1\n",
    "                    print(f\"skipped test: {i}; line: {words} \")\n",
    "                except IndexError:\n",
    "                    print(f\"IndexError at test{i}: {line}\")\n",
    "                    break\n",
    "            accuracy = 100 * float(right_answer) / float(number_tests)\n",
    "            accuracies.append(accuracy)\n",
    "            return accuracy\n",
    "dimensions = [10,100,1000]\n",
    "threshold = 0.00055\n",
    "num_runs = 2\n",
    "results = {}\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"\\n-- experiments for dimension{dim}\")\n",
    "    accuracies = train_word2vec(dim, threshold, num_runs)\n",
    "    results[dim] = accuracies\n",
    "    \n",
    "print(f\"\\n summery of result\")\n",
    "for dim, acc in results.items():\n",
    "    print(f\"dimension{dim}: Avrage Accuracy = {np.mean(acc):2f}%, Runs: {acc}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "                        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konfigurerar loggning för att visa tidsstämpel, loggnivå och meddelandetex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jawad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jawad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 15000\n",
    "ones_number = 2\n",
    "window_size = 2\n",
    "zero_vector = None\n",
    "test_name = \"new_toefl_txt\"\n",
    "data_file_name = r\"c:\\Users\\jawad\\OneDrive\\Skrivbord\\Lab2\\Lab2\\RI\\lemmatized.text\"\n",
    "dimensions = [1000,4000,10000] \n",
    "num_runs =5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen train_evalute tränar och testar en Random Indexing-modell genom att skapa vektorrepresentationer av ord baserat på deras frekvens och grannord i texten. Den utvärderar modellen på TOEFL-uppgifter, beräknar noggrannhet för varje körning och returnerar resultaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evalute(dimension, num_runs):\n",
    "    results = []\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    for run in range(num_runs):\n",
    "        logging.info(f\"Starting run {run + 1}/{num_runs} with dimension {dimension}\")\n",
    "        amount_dictionary = {}\n",
    "        with open(data_file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    amount_dictionary[word] = amount_dictionary.get(word,0)+1\n",
    "        dictionary = {}\n",
    "        with open(data_file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    if word not in dictionary:\n",
    "                        if amount_dictionary[word] < threshold:\n",
    "                            dictionary[word] = train_evalute(dimension, ones_number)\n",
    "                        else:\n",
    "                            dictionary[word] = np.zeros(dimension) \n",
    "        word_space = {}\n",
    "        with open(test_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                words = line.split()\n",
    "                words = [lemmatizer.lemmatize(word) for word in words]\n",
    "                for word in words:\n",
    "                    if word not in word_space:\n",
    "                        word_space[word] = np.zeros(dimension)\n",
    "                        \n",
    "        with open(test_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = [[],[],[],[]]\n",
    "            while len(lines) <4:\n",
    "                line = file.readline()\n",
    "                if line.strip():\n",
    "                    lines.append(line.split())\n",
    "                    \n",
    "            for line in file:\n",
    "                if line.split():\n",
    "                    lines.append(line.split())\n",
    "                    for i, word in enumerate(lines[2]):\n",
    "                        if word in word_space:\n",
    "                            for k in range(1,window_size +1):\n",
    "                                if i - k >= 0:\n",
    "                                    word_space[word] += np.roll(dictionary[line[2][i-k]], -1)\n",
    "                                if i+ k < len(lines[2]):\n",
    "                                    word_space[word] += np.roll(dictionary[line[2][i+k]], 1)\n",
    "                    lines.pop(0)\n",
    "                    \n",
    "        correct_answers = 0\n",
    "        total_testes = 0\n",
    "        with open(test_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                words = line.split()\n",
    "                words = [lemmatizer.lemmatize(word) for word in words]\n",
    "                if words[0] in word_space:\n",
    "                    for i in range(1,5):\n",
    "                        if np.array_equal(word_space[words[i], np.zeros(dimension)]):\n",
    "                            word_space[words[i]] = np.random.randn(dimension)\n",
    "                    correct_answers +=sys.path.append(r\"C:\\Users\\jawad\\OneDrive\\Skrivbord\\Lab2\\Lab2\\GensimW2V\\text_functions\").get_answer_mod([word_space[w] for w in words])\n",
    "                    total_testes+=1\n",
    "                    \n",
    "        accuracy = (correct_answers / total_testes) *100\n",
    "        results.append(accuracy)\n",
    "        logging.info(f\"Run {run + 1} Accuracy: {accuracy:.2f}%\")\n",
    "    return results\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funktionen main tränar och testar modellen för varje dimension, beräknar genomsnittlig noggrannhet och skriver ut resultaten för varje dimension och körning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 13:20:24,855 : INFO : Starting run 1/5 with dimension 1000\n",
      "2024-11-23 13:20:27,837 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:30,642 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:33,463 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:35,545 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:37,608 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:39,680 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:41,758 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:43,901 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:45,966 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:48,006 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:50,044 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:52,070 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:54,099 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:56,112 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:20:58,184 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:00,209 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:02,234 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:04,276 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:06,346 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:08,412 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:10,452 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:12,468 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:14,487 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:16,496 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:18,550 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:20,615 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:22,694 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:24,768 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:26,784 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:28,862 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:30,928 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:33,039 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:35,107 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:37,159 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:39,276 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:41,321 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:43,351 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:45,481 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:47,562 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:49,598 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:51,663 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:53,370 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:55,092 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:56,823 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:21:58,556 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:00,278 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:02,014 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:03,806 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:05,569 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:07,311 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:09,113 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:10,911 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:12,632 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:14,403 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:16,157 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:17,929 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:19,659 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:21,405 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:23,138 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:24,844 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:26,564 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:28,264 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:29,991 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:31,736 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:33,477 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:35,241 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:37,008 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:38,741 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:40,481 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:42,201 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:43,955 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:45,698 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:47,415 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:49,188 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:50,994 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:52,789 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:54,577 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:56,367 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:58,144 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:22:59,906 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:01,638 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:03,374 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:05,150 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:06,886 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:08,661 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:10,429 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:12,209 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:13,977 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:15,744 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:17,557 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:19,474 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:21,304 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:23,133 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:25,006 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:26,906 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:28,698 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:30,466 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:32,241 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:33,980 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:35,763 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:37,511 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:39,274 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:41,042 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:42,792 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:44,556 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:46,338 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:48,123 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:49,875 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:51,607 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:53,352 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:55,118 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:56,903 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:23:58,737 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:00,553 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:02,366 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:04,197 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:06,070 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:07,876 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:09,761 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:11,735 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:13,646 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:15,550 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:17,398 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:19,191 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:20,972 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:22,749 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:24,519 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:26,416 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:28,336 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:30,148 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:32,023 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:33,856 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:35,909 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:37,790 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:39,819 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:41,683 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:43,603 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:45,446 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:47,267 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:49,093 : INFO : Starting run 1/2 with dimension 1000\n",
      "2024-11-23 13:24:50,899 : INFO : Starting run 1/2 with dimension 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndividual accuracies: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m all_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m dimensions:\n\u001b[1;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evalute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     all_results[dim] \u001b[38;5;241m=\u001b[39m results\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Summary of Results ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtrain_evalute\u001b[1;34m(dimension, num_runs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dictionary:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amount_dictionary[word] \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[1;32m---> 19\u001b[0m         dictionary[word] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evalute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mones_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m         dictionary[word] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(dimension) \n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtrain_evalute\u001b[1;34m(dimension, num_runs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dictionary:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amount_dictionary[word] \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[1;32m---> 19\u001b[0m         dictionary[word] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evalute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mones_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m         dictionary[word] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(dimension) \n",
      "    \u001b[1;31m[... skipping similar frames: train_evalute at line 19 (138 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtrain_evalute\u001b[1;34m(dimension, num_runs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dictionary:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amount_dictionary[word] \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[1;32m---> 19\u001b[0m         dictionary[word] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evalute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mones_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m         dictionary[word] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(dimension) \n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mtrain_evalute\u001b[1;34m(dimension, num_runs)\u001b[0m\n\u001b[0;32m      6\u001b[0m amount_dictionary \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    all_results = {}\n",
    "    for dim in dimensions:\n",
    "        results = train_evalute(dim, num_runs)\n",
    "        all_results[dim] = results\n",
    "        \n",
    "    print(\"\\n--- Summary of Results ---\")\n",
    "    for dim, results in all_results.items():\n",
    "        avg_accuracy = np.mean(results)\n",
    "        print(f\"Dimension {dim}: Average Accuracy = {avg_accuracy:.2f}%\")\n",
    "        print(f\"Individual accuracies: {results}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec:\n",
    "\tNoggrannhet: Högre än Random Indexing, särskilt vid högre dimensionalitet (t.ex 100 och 1000).\n",
    "    Beräkningstid och resurser: \n",
    "   Långsammare och mer resurskrävande eftersom det tränar hela vokabulären med neural nätverksbaserade tekniker.\n",
    "Random Indexing:\n",
    "        Noggrannhet: Lägre än Word2Vec, men tillräcklig för enklare uppgifter.\n",
    "        Beräkningstid och resurser: Snabbare och kräver mindre resurser eftersom det endast skapar representationer för relevanta ord.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Förklaring av skillnader\n",
    "    Word2Vec bygger relationer mellan ord genom hela vokabulären, vilket leder till mer precisa representationer men är mer tidskrävande.\n",
    "\n",
    "\n",
    "   Random Indexing skapar representationer med enkla operationer och fokuserar endast på ord i uppgiften, vilket är snabbare men mindre exakt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just nu koden fungerar att alltid använda två ord på vardera sidan ( fönsterstorlek = 2). för att lösa begränsningen att fönsterstorleken är fast kan man istället lägga till en parameter som gör fönsterstorleken justerbar. genom att ändra hur embadding fungerar kan modellen göras mer flexibel och stödja olika fönsterstorlek istället för att vara begränsad till 2.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
